{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\beta$-VAE\n",
    "\n",
    "In this notebook, we're going to implement a $\\beta$-VAE from the paper [beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework](https://openreview.net/forum?id=Sy2fzU9gl). This will require a basic understanding of the VAE, so make sure to check out that notebook first.\n",
    "\n",
    "This is basically an improvement of the VAE, which results in a more disentangled latent space. This means that each dimension of the latent space should impact its own feature of the output. We'll demonstrate this with some experiments on the [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset.\n",
    "\n",
    "All right, let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from torchsummary import summary # TODO: Remove\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "For the encoder, we'll use a convolutional neural network (which often is a good idea to use for image related stuff)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2)\n",
    "        self.fc_mu = nn.Linear(7*7*64, latent_size)\n",
    "        self.fc_logvar = nn.Linear(7*7*64, latent_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.flatten(start_dim=1)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampler\n",
    "\n",
    "We sample using the reparameterization trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(mu, logvar):\n",
    "    std = torch.exp(logvar / 2)\n",
    "    eps = torch.rand_like(std)\n",
    "    return mu + std * eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "For the decoder, we'll use deconvolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.fc = nn.Linear(latent_size, 7*7*64)\n",
    "        self.deconv1 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3, stride=2)\n",
    "        self.deconv2 = nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=3, stride=2)\n",
    "        self.deconv_recon = nn.ConvTranspose2d(in_channels=16, out_channels=3, kernel_size=3, stride=2, output_padding=1)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        z = F.relu(self.fc(z))\n",
    "        z = z.reshape(-1, 64, 7, 7)\n",
    "        z = F.relu(self.deconv1(z))\n",
    "        z = F.relu(self.deconv2(z))\n",
    "        x_recon = torch.sigmoid(self.deconv_recon(z))\n",
    "        return x_recon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE\n",
    "\n",
    "All right, let's put everything together and build a sweet $\\beta$-VAE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(latent_size)\n",
    "        self.decoder = Decoder(latent_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = sample(mu, logvar)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "The only thing that's different from the normal VAE loss is that we introduce a weighting factor $\\beta$ for the Kullback-Leibler divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x_recon, x, mu, logvar, beta=3):\n",
    "    bce = F.binary_cross_entropy(x_recon, x, reduction='sum')\n",
    "    kld = -1/2 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return bce + beta * kld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We'll use the [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset, which contains a bunch of celebrity images. Let's assume that you've extracted this dataset under the directory `data/img_align_celeba/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "test_size = 100\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, paths):\n",
    "        self.paths = paths\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.CenterCrop(178),\n",
    "            transforms.Resize(64),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = datasets.folder.default_loader(self.paths[index])\n",
    "        x = self.transform(x)\n",
    "        return x\n",
    "\n",
    "paths = glob.glob('data/img_align_celeba/*.jpg')\n",
    "\n",
    "train_data = Dataset(paths[:-test_size])\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data = Dataset(paths[-test_size:])\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot some images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid(images, rows, cols):\n",
    "    fig = plt.figure(figsize=(cols*2, rows*2))\n",
    "    for i, image in enumerate(images, start=1):\n",
    "        ax = fig.add_subplot(rows, cols, i)\n",
    "        plt.imshow(image.permute(1, 2, 0), cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "rows = cols = 5\n",
    "indices = np.random.choice(np.arange(len(train_data)), rows * cols, replace=False)\n",
    "images = [train_data[index] for index in indices]\n",
    "plot_grid(images, rows, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now we're ready to train our VAE. We'll use the Adagrad optimization algorithm since that's what's used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = 16\n",
    "\n",
    "vae = VAE(latent_size=latent_size).to(device)\n",
    "optimizer = optim.Adam(vae.parameters())\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0\n",
    "    for batch, x in enumerate(train_loader, start=1):\n",
    "        x = x.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        x_recon, mu, logvar = vae(x)\n",
    "        loss = vae_loss(x_recon, x, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if batch % (len(train_loader) // 10) == 0:\n",
    "            print('=', end='')\n",
    "    print(f'> Epoch: {epoch}, Loss: {running_loss / len(train_data)}')\n",
    "\n",
    "torch.save(vae.state_dict(), 'beta-vae.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to load the saved model\n",
    "#vae.load_state_dict(torch.load('vae.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for x in train_loader:\n",
    "        x_recon, mu, logvar = vae(x)\n",
    "        #plot_grid(sum([[x[i], x_recon[i]] for i, _ in enumerate(x)], []), rows=len(x), cols=2)\n",
    "        \n",
    "        z = sample(mu, logvar)[0]\n",
    "        ls = torch.linspace(-3, 3, steps=5)\n",
    "        images = []\n",
    "        for i, _ in enumerate(z):\n",
    "            for a in ls:\n",
    "                z_ = z.clone()\n",
    "                z_[i] = a\n",
    "                images.append(vae.decoder(z_)[0])\n",
    "        plot_grid(images, rows=len(z), cols=len(images) // len(z))\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right, let's reconstruct some random images and see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 10\n",
    "indices = np.random.choice(np.arange(len(test_data)), cols, replace=False)\n",
    "images = [test_data[index][0].squeeze() for index in indices]\n",
    "with torch.no_grad():\n",
    "    recon = vae(torch.stack(images).reshape(-1, input_size))[0].reshape(-1, *train_data[0][0].shape[1:])\n",
    "    recon = [r for r in recon]\n",
    "plot_grid(images + recon, 2, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're using a 2D latent space, we can make a grid to visualize what the latent space looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = cols = 10\n",
    "with torch.no_grad():\n",
    "    a = torch.linspace(-3, 3, rows)\n",
    "    b = torch.linspace(-3, 3, cols)\n",
    "    z = torch.stack(torch.meshgrid(a, b), dim=2).reshape(-1, latent_size)\n",
    "    samples = vae.decoder(z.to(device))\n",
    "    samples = [sample.reshape(train_data[0][0].shape).squeeze() for sample in samples]\n",
    "    plot_grid(samples, rows, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that the encoder puts different digits in different regions of the latent space. Pretty cool!\n",
    "\n",
    "Well, that's it! If you want to explore the VAE further, you could experiment with using more dimensions for the latent space, and think about how it could be visualized. You could also experiment with using other activation functions, and another optimization algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
